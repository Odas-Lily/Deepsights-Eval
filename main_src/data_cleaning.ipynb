{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/lilian/Documents/THESIS/LLM Eval Project-MLS Deepsight/.venv/lib/python3.13/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/lilian/Documents/THESIS/LLM Eval Project-MLS Deepsight/.venv/lib/python3.13/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/lilian/Documents/THESIS/LLM Eval Project-MLS Deepsight/.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/lilian/Documents/THESIS/LLM Eval Project-MLS Deepsight/.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/lilian/Documents/THESIS/LLM Eval Project-MLS Deepsight/.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/lilian/Documents/THESIS/LLM Eval Project-MLS Deepsight/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context.input</th>\n",
       "      <th>context.summary.answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dog food</td>\n",
       "      <td>### Overview of Dog Food\\n\\nThe dog food marke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Does UK consumers intent to increase of decre...</td>\n",
       "      <td>UK consumers exhibit a mixed intent regarding ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Was sind die Herausforderungen bei der Omnich...</td>\n",
       "      <td>### Herausforderungen bei der Omnichannel in d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Was sind die Herausforderungen für Omnichanne...</td>\n",
       "      <td>### Herausforderungen für Omnichannel in den U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are Gen Z consumption preferences?</td>\n",
       "      <td>Generation Z, defined as individuals born betw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Graph the percent of consumers make food choic...</td>\n",
       "      <td>### Percent of Consumers Making Food Choices B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Graph the percent of consumers make food choic...</td>\n",
       "      <td>To graph the percentage of consumers who make ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Graph the percent of consumers make food choic...</td>\n",
       "      <td>To graph the percentage of consumers who make ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Graph the percent of consumers make food choic...</td>\n",
       "      <td>### Consumer Food Choices Based on Price by Ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Graph the percent of consumers make food choic...</td>\n",
       "      <td>To graph the percentage of consumers who make ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         context.input  \\\n",
       "0                                             dog food   \n",
       "1     Does UK consumers intent to increase of decre...   \n",
       "2     Was sind die Herausforderungen bei der Omnich...   \n",
       "3     Was sind die Herausforderungen für Omnichanne...   \n",
       "4              What are Gen Z consumption preferences?   \n",
       "..                                                 ...   \n",
       "195  Graph the percent of consumers make food choic...   \n",
       "196  Graph the percent of consumers make food choic...   \n",
       "197  Graph the percent of consumers make food choic...   \n",
       "198  Graph the percent of consumers make food choic...   \n",
       "199  Graph the percent of consumers make food choic...   \n",
       "\n",
       "                                context.summary.answer  \n",
       "0    ### Overview of Dog Food\\n\\nThe dog food marke...  \n",
       "1    UK consumers exhibit a mixed intent regarding ...  \n",
       "2    ### Herausforderungen bei der Omnichannel in d...  \n",
       "3    ### Herausforderungen für Omnichannel in den U...  \n",
       "4    Generation Z, defined as individuals born betw...  \n",
       "..                                                 ...  \n",
       "195  ### Percent of Consumers Making Food Choices B...  \n",
       "196  To graph the percentage of consumers who make ...  \n",
       "197  To graph the percentage of consumers who make ...  \n",
       "198  ### Consumer Food Choices Based on Price by Ag...  \n",
       "199  To graph the percentage of consumers who make ...  \n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"question_answer pairs.csv\")\n",
    "\n",
    "# Display the first few rows to understand its structure\n",
    "df.head(200)\n",
    "\n",
    "# import pandas as pd\n",
    "# import json\n",
    "# import csv\n",
    "\n",
    "# #df =pd.read_json('data.json')\n",
    "\n",
    "# df.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted answers saved to 'answers.txt'\n"
     ]
    }
   ],
   "source": [
    "# # Ensure the column exists\n",
    "# column_name = \"context.summary.answer\"\n",
    "# if column_name not in df.columns:\n",
    "#     print(f\"Column '{column_name}' not found. Available columns: {df.columns}\")\n",
    "# else:\n",
    "#     # Convert all values to strings and replace NaN with an empty string\n",
    "#     answers = df[column_name].fillna(\"\").astype(str)\n",
    "\n",
    "#     # Save to a text file\n",
    "#     with open(\"answers.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#         for answer in answers:\n",
    "#             f.write(answer.strip() + \"\\n\\n\")  # Strip whitespace & add spacing\n",
    "\n",
    "#     print(\"Extracted answers saved to 'answers.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique inputs saved to 'update_input.csv'\n"
     ]
    }
   ],
   "source": [
    "# # Extract the 'context.input' column and remove duplicates\n",
    "# unique_answers = df[\"context.input\"].drop_duplicates()\n",
    "\n",
    "# # Save the unique values to a CSV file\n",
    "# unique_answers.to_csv('update_input.csv', index=False)\n",
    "\n",
    "# print(\"Unique inputs saved to 'update_input.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates removed: 2010\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "# file_path = \"/Users/lilian/Documents/THESIS/LLM Eval Project-MLS Deepsight/docs/answers-fmcg-report.csv\"\n",
    "# df = pd.read_csv(file_path)\n",
    "df = pd.read_csv(\"question_answer pairscsv\")\n",
    "\n",
    "# Identify duplicate rows\n",
    "duplicates = df[df.duplicated()]\n",
    "\n",
    "duplicates.head(30)\n",
    "\n",
    "# # Remove duplicates\n",
    "df_cleaned = df.drop_duplicates(subset=[\"question\"])\n",
    "# Save the cleaned file\n",
    "cleaned_file_path = \"/Users/lilian/Documents/THESIS/LLM Eval Project-MLS Deepsight/docs/question_answer pairs.csv\"\n",
    "df_cleaned.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "# Display the number of duplicates removed\n",
    "num_duplicates_removed = len(df) - len(df_cleaned)\n",
    "print(f\"Number of duplicates removed: {num_duplicates_removed}\")\n",
    "\n",
    "# # df_cleaned = df.drop_duplicates(subset=[\"question\"], keep=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 200\n",
      "Number of rows: 200\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"/Users/lilian/Documents/THESIS/LLM Eval Project-MLS Deepsight/data/q&a_for_evaluation.csv\")\n",
    "\n",
    "# Method 1: Using the shape attribute (shape[0] gives the number of rows)\n",
    "num_rows = df.shape[0]\n",
    "print(\"Number of rows:\", num_rows)\n",
    "\n",
    "# Method 2: Using the len() function\n",
    "num_rows = len(df)\n",
    "print(\"Number of rows:\", num_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean merged content written to 'RAG_doc.txt'\n"
     ]
    }
   ],
   "source": [
    "#Cleaning up data used in building the RAG\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel('RAG document.xlsx')\n",
    "\n",
    "# Define the columns to extract\n",
    "columns_to_merge = ['answer','avsTexts', 'srsTexts', 'snsTexts', 'stsTexts']\n",
    "\n",
    "# Check if all columns are present\n",
    "if not all(col in df.columns for col in columns_to_merge):\n",
    "    missing = [col for col in columns_to_merge if col not in df.columns]\n",
    "    raise ValueError(f\"Missing columns in Excel file: {missing}\")\n",
    "\n",
    "# Open the output .txt file\n",
    "with open('RAG_doc', 'w', encoding='utf-8') as f:\n",
    "    for _, row in df.iterrows():\n",
    "        # Get values, skip NaNs and join non-empty parts\n",
    "        parts = [str(row[col]).strip() for col in columns_to_merge if pd.notna(row[col]) and str(row[col]).strip()]\n",
    "        merged = ' '.join(parts)  # join all parts into one line of text\n",
    "        f.write(merged + '\\n\\n')  # extra newline to separate topics\n",
    "\n",
    "print(\"Clean merged content written to 'RAG_doc.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exactly one argument in each of the following groups must be defined: dataset_name, dataset_id",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepsight\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# or your dataset name\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Read existing dataset\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Add examples from the CSV\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/langsmith/utils.py:133\u001b[0m, in \u001b[0;36mxor_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m invalid_groups:\n\u001b[1;32m    132\u001b[0m     invalid_group_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(arg_groups[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m invalid_groups]\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExactly one argument in each of the following\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m groups must be defined:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(invalid_group_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m     )\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: Exactly one argument in each of the following groups must be defined: dataset_name, dataset_id"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "import pandas as pd\n",
    "\n",
    "# Load your CSV\n",
    "file_path = \"/Users/lilian/Documents/LLM Eval Project 2/docs/testfinal.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Initialize LangSmith client\n",
    "client = Client()\n",
    "\n",
    "# Replace with the name of your actual dataset\n",
    "dataset_name = \"deepsight\"  # or your dataset name\n",
    "\n",
    "# Read existing dataset\n",
    "dataset = client.read_dataset(dataset_name)\n",
    "\n",
    "# Add examples from the CSV\n",
    "for _, row in df.iterrows():\n",
    "    inputs = {\"question\": row[\"question\"]}\n",
    "    outputs = {\"answer\": row[\"answer\"]}\n",
    "    \n",
    "    # Create new example in the dataset\n",
    "    client.create_example(inputs=inputs, outputs=outputs, dataset_id=dataset.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
