{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LLM Evaluation Framework**\n",
    "This notebook sets up an evaluation infrastructure for testing LLM performance in a Retrieval-Augmented Generation (RAG) pipeline.\n",
    "### **Key Components**\n",
    "- **Vector Database:** ChromaDB for storing and retrieving documents.\n",
    "- **Embedding Model:** OpenAIEmbeddings for converting text into vector space.\n",
    "- **Retriever:** Queries the vector database for relevant document snippets.\n",
    "- **LLM (GPT-4o-mini, Gemini 2.0 flash, Claude Opus_4):** Generates responses based on retrieved contexts.\n",
    "- **Evaluation Setup:** evaluator to test LLM outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-community in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (0.3.24)\n",
      "Requirement already satisfied: langchain-openai in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (0.3.18)\n",
      "Requirement already satisfied: chromadb in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (1.0.10)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.59 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from langchain-community) (0.3.61)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from langchain-community) (0.3.25)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from langchain-community) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from langchain-community) (3.11.18)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from langchain-community) (2.9.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from langchain-community) (0.3.42)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=2.1.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from langchain-community) (2.2.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (2.11.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (4.13.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: anyio in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: certifi in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from langchain-openai) (1.81.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: build>=1.0.3 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: fastapi==0.115.9 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from chromadb) (0.115.9)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.2)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from chromadb) (4.0.1)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from chromadb) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from chromadb) (1.33.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from chromadb) (1.33.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from chromadb) (0.54b1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from chromadb) (1.33.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from chromadb) (0.21.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from chromadb) (1.71.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from chromadb) (0.15.4)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from chromadb) (32.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from chromadb) (14.0.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from chromadb) (4.23.0)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
      "Requirement already satisfied: pyproject_hooks in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from jsonschema>=4.19.0->chromadb) (0.25.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb) (2.40.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Requirement already satisfied: coloredlogs in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
      "Requirement already satisfied: sympy in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb) (1.17.2)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.33.1 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.33.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.33.1 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.33.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.54b1 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.54b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.54b1 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.54b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.54b1 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.54b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.54b1 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.54b1)\n",
      "Requirement already satisfied: asgiref~=3.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from opentelemetry-instrumentation-asgi==0.54b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from tokenizers>=0.13.2->chromadb) (0.31.4)\n",
      "Requirement already satisfied: filelock in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.5.0)\n",
      "Requirement already satisfied: click<8.2,>=8.0.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-community langchain-openai chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -m venv path/to/venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(69153) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from ipywidgets) (9.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: decorator in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [ipywidgets]\n",
      "\u001b[1A\u001b[2KSuccessfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n"
     ]
    }
   ],
   "source": [
    "! pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip show langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Using cached langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting langchain-openai\n",
      "  Using cached langchain_openai-0.3.17-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting chromadb\n",
      "  Using cached chromadb-1.0.10-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.9 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.59 (from langchain-community)\n",
      "  Using cached langchain_core-0.3.60-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain<1.0.0,>=0.3.25 (from langchain-community)\n",
      "  Using cached langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain-community)\n",
      "  Using cached sqlalchemy-2.0.41-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting requests<3,>=2 (from langchain-community)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain-community)\n",
      "  Using cached PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
      "  Using cached aiohttp-3.11.18-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain-community)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Using cached pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting langsmith<0.4,>=0.1.125 (from langchain-community)\n",
      "  Using cached langsmith-0.3.42-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting numpy>=2.1.0 (from langchain-community)\n",
      "  Using cached numpy-2.2.6-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached frozenlist-1.6.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached multidict-6.4.4-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached propcache-0.3.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached yarl-1.20.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (72 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain<1.0.0,>=0.3.25->langchain-community)\n",
      "  Using cached langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain<1.0.0,>=0.3.25->langchain-community)\n",
      "  Using cached pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.59->langchain-community)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting packaging<25,>=23.2 (from langchain-core<1.0.0,>=0.3.59->langchain-community)\n",
      "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting typing-extensions>=4.7 (from langchain-core<1.0.0,>=0.3.59->langchain-community)\n",
      "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain-community)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.4,>=0.1.125->langchain-community)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.125->langchain-community)\n",
      "  Using cached orjson-3.10.18-cp313-cp313-macosx_15_0_arm64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.125->langchain-community)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.125->langchain-community)\n",
      "  Using cached zstandard-0.23.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community)\n",
      "  Using cached certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community)\n",
      "  Using cached pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain-community)\n",
      "  Using cached charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain-community)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting openai<2.0.0,>=1.68.2 (from langchain-openai)\n",
      "  Using cached openai-1.81.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Using cached tiktoken-0.9.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.68.2->langchain-openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.68.2->langchain-openai)\n",
      "  Using cached jiter-0.10.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai<2.0.0,>=1.68.2->langchain-openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai<2.0.0,>=1.68.2->langchain-openai)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.7->langchain-openai)\n",
      "  Using cached regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Using cached build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting fastapi==0.115.9 (from chromadb)\n",
      "  Using cached fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Using cached posthog-4.0.1-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Using cached onnxruntime-1.22.0-cp313-cp313-macosx_13_0_universal2.whl.metadata (4.5 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_api-1.33.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.33.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Using cached opentelemetry_instrumentation_fastapi-0.54b1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_sdk-1.33.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Using cached PyPika-0.48.9-py2.py3-none-any.whl\n",
      "Collecting overrides>=7.3.1 (from chromadb)\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Using cached grpcio-1.71.0-cp313-cp313-macosx_10_14_universal2.whl.metadata (3.8 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Using cached bcrypt-4.3.0-cp39-abi3-macosx_10_12_universal2.whl.metadata (10 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Using cached typer-0.15.4-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Using cached kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Using cached mmh3-5.1.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (16 kB)\n",
      "Collecting rich>=10.11.0 (from chromadb)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting jsonschema>=4.19.0 (from chromadb)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb)\n",
      "  Using cached starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.19.0->chromadb)\n",
      "  Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.19.0->chromadb)\n",
      "  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=4.19.0->chromadb)\n",
      "  Using cached rpds_py-0.25.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in ./path/to/venv/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./path/to/venv/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached google_auth-2.40.2-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached protobuf-6.31.0-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting importlib-metadata<8.7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached wrapt-1.17.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.33.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.33.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.33.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_proto-1.33.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting opentelemetry-semantic-conventions==0.54b1 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_semantic_conventions-0.54b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.54b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_instrumentation_asgi-0.54b1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation==0.54b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_instrumentation-0.54b1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting opentelemetry-util-http==0.54b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_util_http-0.54b1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.54b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./path/to/venv/lib/python3.13/site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers>=0.13.2->chromadb)\n",
      "  Using cached huggingface_hub-0.31.4-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting filelock (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb)\n",
      "  Using cached fsspec-2025.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting click<8.2,>=8.0.0 (from typer>=0.9.0->chromadb)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached httptools-0.6.4-cp313-cp313-macosx_11_0_arm64.whl.metadata (3.6 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached uvloop-0.21.0-cp313-cp313-macosx_10_13_universal2.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached watchfiles-1.0.5-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached websockets-15.0.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
      "Using cached aiohttp-3.11.18-cp313-cp313-macosx_11_0_arm64.whl (454 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_core-0.3.60-py3-none-any.whl (437 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Using cached langsmith-0.3.42-py3-none-any.whl (360 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading multidict-6.4.4-cp313-cp313-macosx_11_0_arm64.whl (37 kB)\n",
      "Downloading orjson-3.10.18-cp313-cp313-macosx_15_0_arm64.whl (133 kB)\n",
      "Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Downloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl (199 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading sqlalchemy-2.0.41-cp313-cp313-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Downloading yarl-1.20.0-cp313-cp313-macosx_11_0_arm64.whl (94 kB)\n",
      "Using cached zstandard-0.23.0-cp313-cp313-macosx_11_0_arm64.whl (633 kB)\n",
      "Using cached langchain_openai-0.3.17-py3-none-any.whl (62 kB)\n",
      "Downloading openai-1.81.0-py3-none-any.whl (717 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m717.5/717.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.10.0-cp313-cp313-macosx_11_0_arm64.whl (318 kB)\n",
      "Downloading tiktoken-0.9.0-cp313-cp313-macosx_11_0_arm64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Using cached chromadb-1.0.10-cp39-abi3-macosx_11_0_arm64.whl (17.6 MB)\n",
      "Using cached fastapi-0.115.9-py3-none-any.whl (94 kB)\n",
      "Using cached starlette-0.45.3-py3-none-any.whl (71 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached bcrypt-4.3.0-cp39-abi3-macosx_10_12_universal2.whl (498 kB)\n",
      "Using cached build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Downloading frozenlist-1.6.0-cp313-cp313-macosx_11_0_arm64.whl (120 kB)\n",
      "Using cached grpcio-1.71.0-cp313-cp313-macosx_10_14_universal2.whl (11.3 MB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Using cached kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
      "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Downloading google_auth-2.40.2-py2.py3-none-any.whl (216 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached mmh3-5.1.0-cp313-cp313-macosx_11_0_arm64.whl (40 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Using cached numpy-2.2.6-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading onnxruntime-1.22.0-cp313-cp313-macosx_13_0_universal2.whl (34.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.33.1-py3-none-any.whl (65 kB)\n",
      "Downloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
      "Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Using cached wrapt-1.17.2-cp313-cp313-macosx_11_0_arm64.whl (38 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.33.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.33.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.33.1-py3-none-any.whl (55 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading opentelemetry_sdk-1.33.1-py3-none-any.whl (118 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.54b1-py3-none-any.whl (194 kB)\n",
      "Downloading protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.54b1-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.54b1-py3-none-any.whl (31 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.54b1-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_util_http-0.54b1-py3-none-any.whl (7.3 kB)\n",
      "Using cached asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-4.0.1-py2.py3-none-any.whl (92 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading propcache-0.3.1-cp313-cp313-macosx_11_0_arm64.whl (44 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Using cached PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl (171 kB)\n",
      "Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Using cached regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading rpds_py-0.25.1-cp313-cp313-macosx_11_0_arm64.whl (350 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.31.4-py3-none-any.whl (489 kB)\n",
      "Downloading fsspec-2025.5.0-py3-none-any.whl (196 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typer-0.15.4-py3-none-any.whl (45 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "Using cached httptools-0.6.4-cp313-cp313-macosx_11_0_arm64.whl (102 kB)\n",
      "Using cached uvloop-0.21.0-cp313-cp313-macosx_10_13_universal2.whl (1.5 MB)\n",
      "Downloading watchfiles-1.0.5-cp313-cp313-macosx_11_0_arm64.whl (392 kB)\n",
      "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading websockets-15.0.1-cp313-cp313-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: pypika, mpmath, flatbuffers, durationpy, zstandard, zipp, wrapt, websockets, websocket-client, uvloop, urllib3, typing-extensions, tqdm, tenacity, sympy, sniffio, shellingham, rpds-py, regex, PyYAML, python-dotenv, pyproject_hooks, pyasn1, protobuf, propcache, packaging, overrides, orjson, opentelemetry-util-http, oauthlib, numpy, mypy-extensions, multidict, mmh3, mdurl, jsonpointer, jiter, importlib-resources, idna, humanfriendly, httpx-sse, httptools, h11, grpcio, fsspec, frozenlist, filelock, distro, click, charset-normalizer, certifi, cachetools, bcrypt, backoff, attrs, asgiref, annotated-types, aiohappyeyeballs, yarl, uvicorn, typing-inspection, typing-inspect, SQLAlchemy, rsa, requests, referencing, pydantic-core, pyasn1-modules, opentelemetry-proto, marshmallow, markdown-it-py, jsonpatch, importlib-metadata, httpcore, googleapis-common-protos, deprecated, coloredlogs, build, anyio, aiosignal, watchfiles, tiktoken, starlette, rich, requests-toolbelt, requests-oauthlib, pydantic, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, jsonschema-specifications, huggingface-hub, httpx, google-auth, dataclasses-json, aiohttp, typer, tokenizers, pydantic-settings, opentelemetry-semantic-conventions, openai, langsmith, kubernetes, jsonschema, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, langchain-core, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langchain-text-splitters, langchain-openai, opentelemetry-instrumentation-fastapi, langchain, langchain-community, chromadb\n",
      "\u001b[2K  Attempting uninstall: packagingm━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 22/117\u001b[0m [pyasn1]rd]\n",
      "\u001b[2K    Found existing installation: packaging 25.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 22/117\u001b[0m [pyasn1]\n",
      "\u001b[2K    Uninstalling packaging-25.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 22/117\u001b[0m [pyasn1]\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 22/117\u001b[0m [pyasn1]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117/117\u001b[0m [chromadb]chromadb]-community]ore]conventions]\n",
      "\u001b[1A\u001b[2KSuccessfully installed PyYAML-6.0.2 SQLAlchemy-2.0.41 aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 annotated-types-0.7.0 anyio-4.9.0 asgiref-3.8.1 attrs-25.3.0 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 cachetools-5.5.2 certifi-2025.4.26 charset-normalizer-3.4.2 chromadb-1.0.10 click-8.1.8 coloredlogs-15.0.1 dataclasses-json-0.6.7 deprecated-1.2.18 distro-1.9.0 durationpy-0.10 fastapi-0.115.9 filelock-3.18.0 flatbuffers-25.2.10 frozenlist-1.6.0 fsspec-2025.5.0 google-auth-2.40.2 googleapis-common-protos-1.70.0 grpcio-1.71.0 h11-0.16.0 httpcore-1.0.9 httptools-0.6.4 httpx-0.28.1 httpx-sse-0.4.0 huggingface-hub-0.31.4 humanfriendly-10.0 idna-3.10 importlib-metadata-8.6.1 importlib-resources-6.5.2 jiter-0.10.0 jsonpatch-1.33 jsonpointer-3.0.0 jsonschema-4.23.0 jsonschema-specifications-2025.4.1 kubernetes-32.0.1 langchain-0.3.25 langchain-community-0.3.24 langchain-core-0.3.60 langchain-openai-0.3.17 langchain-text-splitters-0.3.8 langsmith-0.3.42 markdown-it-py-3.0.0 marshmallow-3.26.1 mdurl-0.1.2 mmh3-5.1.0 mpmath-1.3.0 multidict-6.4.4 mypy-extensions-1.1.0 numpy-2.2.6 oauthlib-3.2.2 onnxruntime-1.22.0 openai-1.81.0 opentelemetry-api-1.33.1 opentelemetry-exporter-otlp-proto-common-1.33.1 opentelemetry-exporter-otlp-proto-grpc-1.33.1 opentelemetry-instrumentation-0.54b1 opentelemetry-instrumentation-asgi-0.54b1 opentelemetry-instrumentation-fastapi-0.54b1 opentelemetry-proto-1.33.1 opentelemetry-sdk-1.33.1 opentelemetry-semantic-conventions-0.54b1 opentelemetry-util-http-0.54b1 orjson-3.10.18 overrides-7.7.0 packaging-24.2 posthog-4.0.1 propcache-0.3.1 protobuf-5.29.4 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-2.11.4 pydantic-core-2.33.2 pydantic-settings-2.9.1 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.1.0 referencing-0.36.2 regex-2024.11.6 requests-2.32.3 requests-oauthlib-2.0.0 requests-toolbelt-1.0.0 rich-14.0.0 rpds-py-0.25.1 rsa-4.9.1 shellingham-1.5.4 sniffio-1.3.1 starlette-0.45.3 sympy-1.14.0 tenacity-9.1.2 tiktoken-0.9.0 tokenizers-0.21.1 tqdm-4.67.1 typer-0.15.4 typing-extensions-4.13.2 typing-inspect-0.9.0 typing-inspection-0.4.1 urllib3-2.4.0 uvicorn-0.34.2 uvloop-0.21.0 watchfiles-1.0.5 websocket-client-1.8.0 websockets-15.0.1 wrapt-1.17.2 yarl-1.20.0 zipp-3.21.0 zstandard-0.23.0\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain-community langchain-openai chromadb --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(69007) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google.generativeai in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (0.8.5)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from google.generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from google.generativeai) (2.24.2)\n",
      "Requirement already satisfied: google-api-python-client in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from google.generativeai) (2.169.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from google.generativeai) (2.40.2)\n",
      "Requirement already satisfied: protobuf in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from google.generativeai) (5.29.4)\n",
      "Requirement already satisfied: pydantic in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from google.generativeai) (2.11.4)\n",
      "Requirement already satisfied: tqdm in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from google.generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from google.generativeai) (4.13.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from google-ai-generativelanguage==0.6.15->google.generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from google-api-core->google.generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from google-api-core->google.generativeai) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai) (1.72.0rc1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai) (1.71.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from google-auth>=2.15.0->google.generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from google-auth>=2.15.0->google.generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from google-auth>=2.15.0->google.generativeai) (4.9.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (2025.4.26)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google.generativeai) (0.6.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from google-api-python-client->google.generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from google-api-python-client->google.generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from google-api-python-client->google.generativeai) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google.generativeai) (3.2.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from pydantic->google.generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from pydantic->google.generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages (from pydantic->google.generativeai) (0.4.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install google.generativeai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-google-genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.3.25\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /Users/lilian/Documents/LLM Eval Project 2/path/to/venv/lib/python3.13/site-packages\n",
      "Requires: langchain-core, langchain-text-splitters, langsmith, pydantic, PyYAML, requests, SQLAlchemy\n",
      "Required-by: langchain-community\n"
     ]
    }
   ],
   "source": [
    "! pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try brew install\n",
      "\u001b[31m   \u001b[0m xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a Python library that isn't in Homebrew,\n",
      "\u001b[31m   \u001b[0m use a virtual environment:\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m python3 -m venv path/to/venv\n",
      "\u001b[31m   \u001b[0m source path/to/venv/bin/activate\n",
      "\u001b[31m   \u001b[0m python3 -m pip install xyz\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a Python application that isn't in Homebrew,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use 'pipx install xyz', which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. You can install pipx with\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m brew install pipx\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m You may restore the old behavior of pip by passing\n",
      "\u001b[31m   \u001b[0m the '--break-system-packages' flag to pip, or by adding\n",
      "\u001b[31m   \u001b[0m 'break-system-packages = true' to your pip.conf file. The latter\n",
      "\u001b[31m   \u001b[0m will permanently disable this error.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you disable this error, we STRONGLY recommend that you additionally\n",
      "\u001b[31m   \u001b[0m pass the '--user' flag to pip, or set 'user = true' in your pip.conf\n",
      "\u001b[31m   \u001b[0m file. Failure to do this can result in a broken Homebrew installation.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m Read more about this behavior here: <https://peps.python.org/pep-0668/>\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n"
     ]
    }
   ],
   "source": [
    "! python3 -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U langchain-openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain_openai langchain_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_anthropic\n",
      "  Downloading langchain_anthropic-0.3.13-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting anthropic<1,>=0.51.0 (from langchain_anthropic)\n",
      "  Downloading anthropic-0.51.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.59 in ./path/to/venv/lib/python3.13/site-packages (from langchain_anthropic) (0.3.60)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./path/to/venv/lib/python3.13/site-packages (from langchain_anthropic) (2.11.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./path/to/venv/lib/python3.13/site-packages (from anthropic<1,>=0.51.0->langchain_anthropic) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./path/to/venv/lib/python3.13/site-packages (from anthropic<1,>=0.51.0->langchain_anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in ./path/to/venv/lib/python3.13/site-packages (from anthropic<1,>=0.51.0->langchain_anthropic) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./path/to/venv/lib/python3.13/site-packages (from anthropic<1,>=0.51.0->langchain_anthropic) (0.10.0)\n",
      "Requirement already satisfied: sniffio in ./path/to/venv/lib/python3.13/site-packages (from anthropic<1,>=0.51.0->langchain_anthropic) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in ./path/to/venv/lib/python3.13/site-packages (from anthropic<1,>=0.51.0->langchain_anthropic) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in ./path/to/venv/lib/python3.13/site-packages (from anyio<5,>=3.5.0->anthropic<1,>=0.51.0->langchain_anthropic) (3.10)\n",
      "Requirement already satisfied: certifi in ./path/to/venv/lib/python3.13/site-packages (from httpx<1,>=0.25.0->anthropic<1,>=0.51.0->langchain_anthropic) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in ./path/to/venv/lib/python3.13/site-packages (from httpx<1,>=0.25.0->anthropic<1,>=0.51.0->langchain_anthropic) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./path/to/venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic<1,>=0.51.0->langchain_anthropic) (0.16.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.126 in ./path/to/venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.59->langchain_anthropic) (0.3.42)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./path/to/venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.59->langchain_anthropic) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./path/to/venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.59->langchain_anthropic) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./path/to/venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.59->langchain_anthropic) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./path/to/venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.59->langchain_anthropic) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./path/to/venv/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain_anthropic) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./path/to/venv/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.59->langchain_anthropic) (3.10.18)\n",
      "Requirement already satisfied: requests<3,>=2 in ./path/to/venv/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.59->langchain_anthropic) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./path/to/venv/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.59->langchain_anthropic) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./path/to/venv/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.59->langchain_anthropic) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./path/to/venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain_anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./path/to/venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain_anthropic) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./path/to/venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain_anthropic) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./path/to/venv/lib/python3.13/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.59->langchain_anthropic) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./path/to/venv/lib/python3.13/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.59->langchain_anthropic) (2.4.0)\n",
      "Downloading langchain_anthropic-0.3.13-py3-none-any.whl (26 kB)\n",
      "Downloading anthropic-0.51.0-py3-none-any.whl (263 kB)\n",
      "Installing collected packages: anthropic, langchain_anthropic\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [langchain_anthropic]\n",
      "\u001b[1A\u001b[2KSuccessfully installed anthropic-0.51.0 langchain_anthropic-0.3.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip show langchain chromadb langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_anthropic import ChatAnthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env\n",
    "#load_dotenv(dotenv_path='.env', override=True)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import utils\n",
    "utils.tracing_is_enabled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = os.getenv(\"LANGSMITH_TRACING\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = str(os.getenv(\"MLS-eval\"))\n",
    "os.environ['LANGCHAIN_ENDPOINT']= os.getenv(\"LANGSMITH_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the document\n",
    "doc_path = os.getcwd()\n",
    "dir = os.path.dirname(os.path.abspath(doc_path))\n",
    "file_path = os.path.join(dir, \"docs\", \"actual_data.txt\")\n",
    "\n",
    "# Load the document using TextLoader\n",
    "loader = TextLoader(file_path)\n",
    "documents = loader.load()   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the Document into Chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document into smaller chunks for processing(chunks of 300 characters with 20 characters overlap)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embed the Document Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mb/m74ql8px72lfmqmlm4_rg36m0000gn/T/ipykernel_51192/3368382630.py:32: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(query)\n",
      "Python(68294) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1:\n",
      "1. **Fresh Dog Food**: \n",
      "   - Fresh dog food is gaining popularity, characterized by whole-food ingredients and often delivered via subscription services. Brands like JustFoodForDogs and Nom Nom are noted for their adherence to nutritional standards set by the Association of American Feed Control Off\n",
      "\n",
      "Result 2:\n",
      "- **Fresh Dog Food**: This category has gained popularity, with brands like JustFoodForDogs and Nom Nom offering meals made from whole ingredients. These foods are typically delivered to consumers and are marketed as healthier alternatives to traditional kibble. However, experts caution that fresh f\n",
      "\n",
      "Result 3:\n",
      "#### 3. Product Trends\n",
      "- Fresh dog food is gaining popularity, with many brands offering subscription services that deliver meals made from whole-food ingredients directly to consumers' homes. Notable brands include JustFoodForDogs and Nom Nom, which meet the standards set by the Association of Amer\n",
      "\n",
      "Result 4:\n",
      "medicine at Small Door Veterinary. You don't need to break the bank to feed your dog a healthy diet.\\nWe've found plenty of high-quality foods available at lower prices. As long as a food meets the AAFCO complete and balanced standards like our picks and makes sense for your dog's life stage, you're\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Load text content from the file\n",
    "file_path = \"/Users/lilian/Documents/LLM Eval Project 2/data/merged.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    full_text = file.read()\n",
    "\n",
    "# Split into chunks using LangChain's text splitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "documents = splitter.create_documents([full_text])\n",
    "\n",
    "# Define Chroma persist directory\n",
    "persist_dir = \"./chroma_db\"\n",
    "\n",
    "# Create embeddings and store in Chroma\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_dir\n",
    ")\n",
    "# vectorstore.persist()\n",
    "\n",
    "# Get retriever from vectorstore\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Example query\n",
    "query = \"Dog food?\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Print top results\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\nResult {i+1}:\\n{doc.page_content[:300]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1:\n",
      "### Overview of Dog Food\n",
      "\n",
      "Result 2:\n",
      "Dog food is a vital component of pet care, designed to meet the specific dietary needs of dogs. With various types available, including dry, wet, and fresh options, it is essential for pet owners to choose products that adhere to established nutritional standards to ensure their pets' health and wel\n",
      "\n",
      "Result 3:\n",
      "### Nutritional Guidelines for Dog Food\n",
      "\n",
      "Result 4:\n",
      "1. **Fresh Dog Food**: \n",
      "   - Fresh dog food is gaining popularity, characterized by whole-food ingredients and often delivered via subscription services. Brands like JustFoodForDogs and Nom Nom are noted for their adherence to nutritional standards set by the Association of American Feed Control Off\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "persist_dir = \"./chroma_db\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Load existing store\n",
    "vectorstore = Chroma(persist_directory=persist_dir, embedding_function=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "query = \"Dog food\"\n",
    "docs = retriever.invoke(query)  # or .get_relevant_documents() if < langchain-core 0.1.46\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\nResult {i+1}:\\n{doc.page_content[:300]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1:\n",
      " ズや優先事項に対応するために、ツールは毎年更新されています。これらの更新のおかげで、ツールは最も正確なデータを確実に提供し、持続可能なビジネス上の意思決定を推進し、さらに、既存および新たに発生する報告義務の遵守をサポートすることができるのです。2024年第4四半期には、Higg\n",
      "\n",
      "Result 2:\n",
      " ズや優先事項に対応するために、ツールは毎年更新されています。これらの更新のおかげで、ツールは最も正確なデータを確実に提供し、持続可能なビジネス上の意思決定を推進し、さらに、既存および新たに発生する報告義務の遵守をサポートすることができるのです。2024年第4四半期には、Higg\n",
      "\n",
      "Result 3:\n",
      " ズや優先事項に対応するために、ツールは毎年更新されています。これらの更新のおかげで、ツールは最も正確なデータを確実に提供し、持続可能なビジネス上の意思決定を推進し、さらに、既存および新たに発生する報告義務の遵守をサポートすることができるのです。2024年第4四半期には、Higg\n",
      "\n",
      "Result 4:\n",
      " ズや優先事項に対応するために、ツールは毎年更新されています。これらの更新のおかげで、ツールは最も正確なデータを確実に提供し、持続可能なビジネス上の意思決定を推進し、さらに、既存および新たに発生する報告義務の遵守をサポートすることができるのです。2024年第4四半期には、Higg\n"
     ]
    }
   ],
   "source": [
    "# Query test\n",
    "query = \"サステナビリティに関する最新のトレンドは何ですか?\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Print results\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\nResult {i+1}:\\n\", doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(\n",
    "    persist_directory=\"./chroma_db\", \n",
    "    embedding_function=embeddings\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Additional Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import datetime\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Get today's date in YYYY-MM-DD format\n",
    "today = datetime.datetime.now().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the RAG Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Rag:\n",
    "    #def __init__(self, retriever, model: str = \"gpt-4o\"):\n",
    "    def __init__(self, retriever, model: str = \"gpt-4o-mini\"):\n",
    "    # def __init__(self, retriever, model: str = \"gpt-3.5-turbo\"):\n",
    "        \"\"\"\n",
    "        Initialize the RAG (Retrieval-Augmented Generation) model.\n",
    "        \n",
    "        Args:\n",
    "            retriever: The retriever object used to fetch relevant document chunks.\n",
    "            model (str): The name of the LLM model to use (default: \"gpt-4o\").\n",
    "        \"\"\"        \n",
    "        self._retriever = retriever\n",
    "        \n",
    "        # Wrap the OpenAI client to enable tracing and loggin\n",
    "        self._client = wrap_openai(openai.Client())\n",
    "        #self._client = genai.GenerativeModel(model)\n",
    "\n",
    "        # Set the LLM model to use\n",
    "        self._model = model\n",
    "\n",
    "    #CONTEXT = \"answer business questions based on provided documents\"\n",
    "\n",
    "    @traceable # Decorator to trace the function call for monitoring and debugging\n",
    "    def get_answer(self, question: str):\n",
    "        \"\"\"\n",
    "        Generate an answer to a question using the RAG model.\n",
    "        \n",
    "        Args:\n",
    "            question (str): The question to answer.\n",
    "        \n",
    "        Returns:\n",
    "            dict: A dictionary containing the generated answer and the contexts used.\n",
    "        \"\"\"    \n",
    "\n",
    "        # Retrieve relevant document chunks based on the question    \n",
    "        similar = self._retriever.invoke(question)\n",
    "        \n",
    "        response = self._client.chat.completions.create(\n",
    "        #response = self._client.generate_content(\n",
    "    \n",
    "            model=self._model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an accomplished AI working for the insights department at a company.Your job is to answer business questions based on provided documents. \"\n",
    "                    \"Today is {today}.\"\n",
    "                               \" Use the following docs to produce a concise answer to the users question\"\n",
    "                               f\"## Docs\\n\\n{similar}\"\n",
    "                     \"\"\"Please follow these steps to provide a response:\n",
    "\n",
    "                    1. Carefully review the source material, paying attention to any information that is relevant to answering the question:\n",
    "                        - Make extensive use of all information given.\n",
    "                        - It is CRITICAL that you only use information that is explicitly stated above.\n",
    "                        - Refrain from recommendations, speculation, or extrapolations.\n",
    "                        - Compare and contrast findings from different sources. Watch out for any apparent conflicts between sources as well as any corroborating information.\n",
    "                        - Make sure the context of the information given is applicable to the question, such as any specific country, category, or target group the question asks about.\n",
    "\n",
    "                    2. Write the answer in a professional, well-structured format with headings and inline citations:\n",
    "                        - Make sure to write a didactically well-structured answer for insights professionals and business stakeholders.\n",
    "                        - Aim to provide a professional response that will help inform decision-making.\n",
    "                        - Structure your answer with headings and use concise full text.\n",
    "                        - Use tables in your response only when needed.\n",
    "                        - Use lists and bullet points sparingly. Absolutely avoid nesting lists and bullet points.\n",
    "                        - Ensure to include direct inline citations for all information referenced in your answer. Use a bracketed citation style with source reference like [XXX]. If there are multiple references, provide them in separate brackets each: [XXX][ZZZ]. MAKE SURE TO USE THE REFERENCES EXACTLY AS GIVEN IN THE <reference> TAGS ABOVE.\n",
    "                    \n",
    "                    3. **Language**: Respond in the SAME LANGUAGE as the user's question or the provided documents. If the question/documents are in Spanish, reply in Spanish. If in German, reply in German, etc.\n",
    "\n",
    "                    Make sure to use Markdown in your response and structure it in this format:\n",
    "\n",
    "                    <answer>\n",
    "                    ...\n",
    "                    </answer>\n",
    "            \"\"\"},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Return the generated answer and the contexts used\n",
    "        return {\n",
    "            #\"answer\": response.text,\n",
    "            \"answer\": response.choices[0].message.content, # Extract the generated answer\n",
    "            \"contexts\": [str(doc) for doc in similar], # Convert document chunks to strings\n",
    "        }\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from langsmith import traceable\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import google.generativeai as genai\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Get today's date in YYYY-MM-DD format\n",
    "today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "class Rag:\n",
    "    def __init__(self, retriever, model: str = \"gemini-2.0-flash\"):\n",
    "        \"\"\"\n",
    "        Initialize the RAG (Retrieval-Augmented Generation) model.\n",
    "\n",
    "        Args:\n",
    "            retriever: The retriever object used to fetch relevant document chunks.\n",
    "            model (str): The name of the LLM model to use (default: \"gemini-1.5-flash\").\n",
    "        \"\"\"\n",
    "        self._retriever = retriever\n",
    "\n",
    "        # Initialize the Gemini model\n",
    "        self._model = ChatGoogleGenerativeAI(model=model, temperature=0.0)   \n",
    "\n",
    "    @traceable  # Decorator to trace the function call for monitoring and debugging\n",
    "    def get_answer(self, question: str):\n",
    "        \"\"\"\n",
    "        Generate an answer to a question using the RAG model.\n",
    "\n",
    "        Args:\n",
    "            question (str): The question to answer.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the generated answer and the contexts used.\n",
    "        \"\"\"\n",
    "\n",
    "        # Retrieve relevant document chunks based on the question\n",
    "        similar = self._retriever.invoke(question)\n",
    "\n",
    "        # Prepare the system message and user message for the chat model\n",
    "        system_message = SystemMessage(\n",
    "            content=f\"\"\"You are an accomplished AI working for the insights department at a company. Your job is to answer business questions based on provided documents.\n",
    "Today is {today}.\n",
    "Use the following docs to produce a concise answer to the user's question:\n",
    "## Docs\\n\\n{similar}\n",
    "\n",
    "Please follow these steps to provide a response:\n",
    "\n",
    "1. Carefully review the source material, paying attention to any information that is relevant to answering the question:\n",
    "    - Make extensive use of all information given.\n",
    "    - It is CRITICAL that you only use information that is explicitly stated above.\n",
    "    - Refrain from recommendations, speculation, or extrapolations.\n",
    "    - Compare and contrast findings from different sources. Watch out for any apparent conflicts between sources as well as any corroborating information.\n",
    "    - Make sure the context of the information given is applicable to the question, such as any specific country, category, or target group the question asks about.\n",
    "\n",
    "2. Write the answer in a professional, well-structured format with headings and inline citations:\n",
    "    - Make sure to write a didactically well-structured answer for insights professionals and business stakeholders.\n",
    "    - Aim to provide a professional response that will help inform decision-making.\n",
    "    - Structure your answer with headings and use concise full text.\n",
    "    - Use tables in your response only when needed.\n",
    "    - Use lists and bullet points sparingly. Absolutely avoid nesting lists and bullet points.\n",
    "    - Ensure to include direct inline citations for all information referenced in your answer. Use a bracketed citation style with source reference like [XXX]. If there are multiple references, provide them in separate brackets each: [XXX][ZZZ]. MAKE SURE TO USE THE REFERENCES EXACTLY AS GIVEN IN THE <reference> TAGS ABOVE.\n",
    "\n",
    "3. Language: Respond in the SAME LANGUAGE as the user's question or the provided documents. If the question/documents are in Spanish, reply in Spanish. If in German, reply in German, etc.\n",
    "\n",
    "Make sure to use Markdown in your response and structure it in this format:\n",
    "\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "        )\n",
    "        user_message = HumanMessage(content=question)\n",
    "\n",
    "        # Generate the response using the Gemini model\n",
    "        response = self._model.invoke([system_message, user_message])\n",
    "\n",
    "        # Return the generated answer and the contexts used\n",
    "        return {\n",
    "            \"answer\": response.content,  # Extract the generated answer\n",
    "            \"contexts\": [str(doc) for doc in similar],  # Convert document chunks to strings\n",
    "        }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rag:\n",
    "    def __init__(self, retriever, model: str = \"claude-opus-4-20250514\"):\n",
    "        \"\"\"\n",
    "        Initialize the RAG (Retrieval-Augmented Generation) model.\n",
    "\n",
    "        Args:\n",
    "            retriever: The retriever object used to fetch relevant document chunks.\n",
    "            model (str): The name of the Anthropic model to use (default: \"claude-3-sonnet-20240229\").\n",
    "        \"\"\"\n",
    "        self._retriever = retriever\n",
    "\n",
    "        # Initialize the Anthropic model\n",
    "        self._model = ChatAnthropic(model=model)\n",
    "\n",
    "    @traceable  # Decorator to trace the function call for monitoring and debugging\n",
    "    def get_answer(self, question: str):\n",
    "        \"\"\"\n",
    "        Generate an answer to a question using the RAG model.\n",
    "\n",
    "        Args:\n",
    "            question (str): The question to answer.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the generated answer and the contexts used.\n",
    "        \"\"\"\n",
    "\n",
    "        # Retrieve relevant document chunks based on the question\n",
    "        similar = self._retriever.invoke(question)\n",
    "\n",
    "        # Prepare the system message and user message for the chat model\n",
    "        system_message = SystemMessage(\n",
    "            content=f\"\"\"You are an accomplished AI working for the insights department at a company. Your job is to answer business questions based on provided documents.\n",
    "Today is {today}.\n",
    "Use the following docs to produce a concise answer to the user's question:\n",
    "## Docs\\n\\n{similar}\n",
    "\n",
    "Please follow these steps to provide a response:\n",
    "\n",
    "1. Carefully review the source material, paying attention to any information that is relevant to answering the question:\n",
    "    - Make extensive use of all information given.\n",
    "    - It is CRITICAL that you only use information that is explicitly stated above.\n",
    "    - Refrain from recommendations, speculation, or extrapolations.\n",
    "    - Compare and contrast findings from different sources. Watch out for any apparent conflicts between sources as well as any corroborating information.\n",
    "    - Make sure the context of the information given is applicable to the question, such as any specific country, category, or target group the question asks about.\n",
    "\n",
    "2. Write the answer in a professional, well-structured format with headings and inline citations:\n",
    "    - Make sure to write a didactically well-structured answer for insights professionals and business stakeholders.\n",
    "    - Aim to provide a professional response that will help inform decision-making.\n",
    "    - Structure your answer with headings and use concise full text.\n",
    "    - Use tables in your response only when needed.\n",
    "    - Use lists and bullet points sparingly. Absolutely avoid nesting lists and bullet points.\n",
    "    - Ensure to include direct inline citations for all information referenced in your answer. Use a bracketed citation style with source reference like [XXX]. If there are multiple references, provide them in separate brackets each: [XXX][ZZZ]. MAKE SURE TO USE THE REFERENCES EXACTLY AS GIVEN IN THE <reference> TAGS ABOVE.\n",
    "\n",
    "3. Language: Respond in the SAME LANGUAGE as the user's question or the provided documents. If the question/documents are in Spanish, reply in Spanish. If in German, reply in German, etc.\n",
    "\n",
    "Make sure to use Markdown in your response and structure it in this format:\n",
    "\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "        )\n",
    "        user_message = HumanMessage(content=question)\n",
    "\n",
    "        # Generate the response using the Anthropic model\n",
    "        response = self._model.invoke([system_message, user_message])\n",
    "\n",
    "        # Return the generated answer and the contexts used\n",
    "        return {\n",
    "            \"answer\": response.content,  # Extract the generated answer\n",
    "            \"contexts\": [str(doc) for doc in similar],  # Convert document chunks to strings\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Initialize the RAG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (Ensure `retriever` is properly initialized)\n",
    "rag = Rag(retriever) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate an Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer: <answer>\n",
      "# The Color of Green Tea\n",
      "\n",
      "## Primary Color Characteristics\n",
      "\n",
      "Green tea is characterized by its **light green to yellow-green color** [439e0944-9e01-4b7a-9ed8-be74ec0f343b]. This distinctive coloration is primarily due to the presence of chlorophyll, which is retained during the processing of the tea leaves [439e0944-9e01-4b7a-9ed8-be74ec0f343b].\n",
      "\n",
      "## Color Variations\n",
      "\n",
      "The specific shade of green tea can vary depending on several factors [439e0944-9e01-4b7a-9ed8-be74ec0f343b]:\n",
      "- Type of green tea\n",
      "- Processing methods used\n",
      "- Some varieties may appear more yellowish\n",
      "\n",
      "## Processing Impact on Color\n",
      "\n",
      "Unlike black tea, which undergoes full oxidation and turns dark, green tea is minimally processed, allowing it to maintain its **vibrant green hue** [439e0944-9e01-4b7a-9ed8-be74ec0f343b]. This minimal processing is crucial in preserving the characteristic green color of the tea.\n",
      "\n",
      "## Additional Color References\n",
      "\n",
      "The documents also mention beverages containing green tea ingredients, including a \"teal-colored beverage\" [e1c189db-a774-4917-8daa-71f59a247e6b] and references to \"lemon matcha + green tea\" [e1c189db-a774-4917-8daa-71f59a247e6b], though these appear to be processed or mixed beverage products rather than pure green tea.\n",
      "</answer>\n",
      "Contexts Used: ['page_content=\\'In the context of the source material, while it does not directly address the color of green tea, it does mention various types of green teas, such as \"Meecha,\" \"Gunpowder,\" and \"Longing,\" which are known for their distinct appearances and flavors, further emphasizing the diversity within green tea itself [01-01]. [\"# FIGURE 1 Major tea polyphenols and their chemical structures.\\\\n\\\\nA figure showing the chemical structures of various tea polyphenols: Catechin, Gallocatechin, Catechin-3-gallate,\\'', \"page_content='Green tea is characterized by its light green to yellow-green color. This color is primarily due to the presence of chlorophyll, which is retained during the processing of the tea leaves. Unlike black tea, which undergoes full oxidation and turns dark, green tea is minimally processed, allowing it to maintain its vibrant green hue. The specific shade can vary depending on the type of green tea and the processing methods used. For instance, some varieties may appear more yellowish or even'\", 'page_content=\\'a can of unsweetened green tea, and a bottle of ice-steeped cold brew lemon matcha + green tea.)\\\\n\\\\nCopyright © 2017 Beverage Marketing Corp.\\\\n\\\\nBeverage Marketing Corporation\",\"# Tea Time\\\\n\\\\n## New global flavors win consumers\\' preference\\\\n\\\\nA woman with short blonde hair is smiling while drinking a teal-colored beverage topped with light green cotton candy. She is wearing a light blue eyelet dress.\\\\n\\\\nThe widely recognized health benefits of tea have long driven consumer experimentation with\\'', 'page_content=\\'lemon matcha + green tea.)\\\\n\\\\nCopyright © 2017 Beverage Marketing Corp.\\\\n\\\\nBeverage Marketing Corporation\",\"# Tea Time\\\\n\\\\n## New global flavors win consumers\\' preference\\\\n\\\\nA woman with short blonde hair is smiling while drinking a teal-colored beverage topped with light green cotton candy. She is wearing a light blue eyelet dress.\\\\n\\\\nThe widely recognized health benefits of tea have long driven consumer experimentation with this ancient and beloved beverage, but we\\'ve identified several\\'']\n"
     ]
    }
   ],
   "source": [
    "response = rag.get_answer(\"what is the colour of green tea from the document\")\n",
    "print(\"Generated Answer:\", response[\"answer\"])\n",
    "print(\"Contexts Used:\", response[\"contexts\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------\n",
    "def predict_rag_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    response = rag.get_answer(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"]}\n",
    "\n",
    "def predict_rag_answer_with_context(example: dict):\n",
    "    \"\"\"Use this for evaluation of retrieved documents and hallucinations\"\"\"\n",
    "    response = rag.get_answer(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"], \"contexts\": response[\"contexts\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the QA Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "# Define the QA evaluator\n",
    "qa_evaluator = [\n",
    "    LangChainStringEvaluator(\n",
    "        \"cot_qa\", # Use the Chain-of-Thought QA evaluator\n",
    "        prepare_data=lambda run, example: {\n",
    "            \"prediction\": run.outputs[\"answer\"], # Predicted answer from the RAG model\n",
    "            \"reference\": example.outputs[\"answer\"], # Ground truth answer\n",
    "            \"input\": example.inputs[\"question\"], # Input question\n",
    "        }\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade output schema\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class RelevanceGrade(TypedDict):\n",
    "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
    "    relevant: Annotated[bool, ..., \"Provide the score on whether the answer addresses the question\"]\n",
    "\n",
    "# Grade prompt\n",
    "relevance_instructions=\"\"\"You are a teacher grading a quiz. \n",
    "\n",
    "You will be given a QUESTION and a set of FACTS provided by the student. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "(1) You goal is to identify FACTS that are completely unrelated to the QUESTION\n",
    "(2) If the facts contain ANY keywords or semantic meaning related to the question, consider them relevant\n",
    "(3) It is OK if the facts have SOME information that is unrelated to the question (2) is met \n",
    "\n",
    "Score:\n",
    "A score of 1 means that the FACT contain ANY keywords or semantic meaning related to the QUESTION and are therefore relevant. This is the highest (best) score. \n",
    "A score of 0 means that the FACTS are completely unrelated to the QUESTION. This is the lowest possible score you can give.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Grader LLM\n",
    "#relevance_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0).with_structured_output(RelevanceGrade, method=\"json_schema\", strict=True)\n",
    "#relevance_llm = ChatGoogleGenerativeAI(model= \"gemini-2.0-flash\", temperature=0).with_structured_output(RelevanceGrade, method=\"json_schema\", strict=True)\n",
    "relevance_llm = ChatAnthropic(model= \"claude-opus-4-20250514\", temperature=0).with_structured_output(RelevanceGrade, method=\"json_schema\", strict=True)\n",
    "\n",
    "# Evaluator\n",
    "def relevance(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"A simple evaluator for RAG answer helpfulness.\"\"\"\n",
    "    answer = f\"QUESTION: {inputs['question']}\\nSTUDENT ANSWER: {outputs['answer']}\"\n",
    "    grade = relevance_llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": relevance_instructions}, \n",
    "        {\"role\": \"user\", \"content\": answer}\n",
    "    ])\n",
    "    return grade[\"relevant\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'claude-Opus-4-ee4e74f6' at:\n",
      "https://smith.langchain.com/o/becb2dbe-6434-46cc-a3fb-97dca03fd3ef/datasets/0602a3af-66bf-4930-9881-f7cc04698b95/compare?selectedSessions=18e13b5b-1b32-4dc4-a05f-123755d2325c\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3daa39f00f4984abcd58cc2be94f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running evaluator <DynamicRunEvaluator relevance> on run 8b6251b6-d0d5-4bef-970d-013607c7d5c6: InternalServerError(\"Error code: 500 - {'type': 'error', 'error': {'type': 'api_error', 'message': 'Internal server error'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1627, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"/Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 343, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"/Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 741, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/var/folders/mb/m74ql8px72lfmqmlm4_rg36m0000gn/T/ipykernel_46268/22438521.py\", line 38, in relevance\n",
      "    grade = relevance_llm.invoke([\n",
      "        {\"role\": \"system\", \"content\": relevance_instructions},\n",
      "        {\"role\": \"user\", \"content\": answer}\n",
      "    ])\n",
      "  File \"/Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py\", line 3045, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config, **kwargs)\n",
      "  File \"/Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py\", line 5430, in invoke\n",
      "    return self.bound.invoke(\n",
      "           ~~~~~~~~~~~~~~~~~^\n",
      "        input,\n",
      "        ^^^^^^\n",
      "        self._merge_configs(config),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        **{**self.kwargs, **kwargs},\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 371, in invoke\n",
      "    self.generate_prompt(\n",
      "    ~~~~~~~~~~~~~~~~~~~~^\n",
      "        [self._convert_input(input)],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<6 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    ).generations[0][0],\n",
      "    ^\n",
      "  File \"/Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 956, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 775, in generate\n",
      "    self._generate_with_cache(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        m,\n",
      "        ^^\n",
      "    ...<2 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1021, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
      "    )\n",
      "  File \"/Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages/langchain_anthropic/chat_models.py\", line 1204, in _generate\n",
      "    data = self._client.messages.create(**payload)\n",
      "  File \"/Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages/anthropic/_utils/_utils.py\", line 283, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages/anthropic/resources/messages/messages.py\", line 978, in create\n",
      "    return self._post(\n",
      "           ~~~~~~~~~~^\n",
      "        \"/v1/messages\",\n",
      "        ^^^^^^^^^^^^^^^\n",
      "    ...<26 lines>...\n",
      "        stream_cls=Stream[RawMessageStreamEvent],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages/anthropic/_base_client.py\", line 1290, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/lilian/Documents/LLM Eval Project 2/.venv/lib/python3.13/site-packages/anthropic/_base_client.py\", line 1085, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "anthropic.InternalServerError: Error code: 500 - {'type': 'error', 'error': {'type': 'api_error', 'message': 'Internal server error'}}\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "# Define the dataset name\n",
    "dataset_name = \"mls-deepsight-eval\"\n",
    "\n",
    "\n",
    "# Evaluator\n",
    "cot_qa_evaluator = LangChainStringEvaluator(\n",
    "        \"cot_qa\",\n",
    "        prepare_data=lambda run, example: {\n",
    "            \"prediction\": run.outputs[\"answer\"],\n",
    "            \"reference\": example.outputs[\"answer\"],\n",
    "            \"input\": example.inputs[\"question\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators= [cot_qa_evaluator, relevance],\n",
    "    experiment_prefix=\"claude-Opus-4\",\n",
    "    num_repetitions=5,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators= [cot_qa_evaluator, relevance],\n",
    "    experiment_prefix=\"gpt-4o-min\",\n",
    "    num_repetitions=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators= [cot_qa_evaluator, relevance],\n",
    "    experiment_prefix=\"gemini-2.0-flash\",\n",
    "    num_repetitions=4,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
