{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec92c3c0",
   "metadata": {},
   "source": [
    "\n",
    "## **LLM Evaluation Framework**\n",
    "This notebook sets up an evaluation infrastructure for testing LLM performance in a Retrieval-Augmented Generation (RAG) pipeline.\n",
    "### **Key Components**\n",
    "- **Vector Database:** ChromaDB for storing and retrieving documents.\n",
    "- **Embedding Model:** OpenAIEmbeddings for converting text into vector space.\n",
    "- **Retriever:** Queries the vector database for relevant document snippets.\n",
    "- **LLM (GPT-4o, GPT-3.5, Gemini):** Generates responses based on retrieved contexts.\n",
    "- **Evaluation Setup:** Dummy evaluator to test LLM outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install langchain_community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -qU langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error loading /Users/lilian/docs/dummy.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/langchain_community/document_loaders/text.py:42\u001b[0m, in \u001b[0;36mTextLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     43\u001b[0m         text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/lilian/docs/dummy.txt'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mdir\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdummy.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m loader \u001b[38;5;241m=\u001b[39m TextLoader(file_path)\n\u001b[0;32m---> 14\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Split\u001b[39;00m\n\u001b[1;32m     17\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/langchain_core/document_loaders/base.py:31\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/langchain_community/document_loaders/text.py:58\u001b[0m, in \u001b[0;36mTextLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     60\u001b[0m metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)}\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m Document(page_content\u001b[38;5;241m=\u001b[39mtext, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error loading /Users/lilian/docs/dummy.txt"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "\n",
    "\n",
    "doc_path = os.getcwd()\n",
    "dir = os.path.dirname(os.path.abspath(doc_path))\n",
    "file_path = os.path.join(dir, \"docs\", \"dummy.txt\")\n",
    "\n",
    "loader = TextLoader(file_path)\n",
    "documents = loader.load()   \n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=20)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Index\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import datetime\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "#from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "#from langchain_google_genai import GoogleGenerativeAI\n",
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "class Ragdummy:\n",
    "    def __init__(self, retriever, model: str = \"gpt-4o\"):\n",
    "    #def __init__(self, retriever, model: str = \"gpt-4o-mini\"):\n",
    "    #def __init__(self, retriever, model: str = \"gpt-3.5-turbo\"):\n",
    "    #def __init__(self, retriever, model: str = \"gemini-2.0-flash\"):    \n",
    "        self._retriever = retriever\n",
    "        # Wrapping the client instruments the LLM\n",
    "        self._client = wrap_openai(openai.Client())\n",
    "        #self._client = genai.GenerativeModel(model)\n",
    "        self._model = model\n",
    "\n",
    "    #CONTEXT = \"answer business questions based on provided documents\"\n",
    "    @traceable\n",
    "    def get_answer(self, question: str):\n",
    "        similar = self._retriever.invoke(question)\n",
    "        \n",
    "        response = self._client.chat.completions.create(\n",
    "        #response = self._client.generate_content(\n",
    "    \n",
    "            model=self._model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an accomplished AI working for the insights department at a company.Your job is to answer business questions based on provided documents. \"\n",
    "                    \"Today is {today}.\"\n",
    "                               \" Use the following docs to produce a concise answer to the users question\"\n",
    "                               f\"## Docs\\n\\n{similar}\"\n",
    "                     \"\"\"Please follow these steps to provide a response:\n",
    "\n",
    "                    1. Carefully review the source material, paying attention to any information that is relevant to answering the question:\n",
    "                        - Make extensive use of all information given.\n",
    "                        - It is CRITICAL that you only use information that is explicitly stated above.\n",
    "                        - Refrain from recommendations, speculation, or extrapolations.\n",
    "                        - Compare and contrast findings from different sources. Watch out for any apparent conflicts between sources as well as any corroborating information.\n",
    "                        - Make sure the context of the information given is applicable to the question, such as any specific country, category, or target group the question asks about.\n",
    "\n",
    "                    2. Write the answer in a professional, well-structured format with headings and inline citations:\n",
    "                        - Make sure to write a didactically well-structured answer for insights professionals and business stakeholders.\n",
    "                        - Aim to provide a professional response that will help inform decision-making.\n",
    "                        - Structure your answer with headings and use concise full text.\n",
    "                        - Use tables in your response only when needed.\n",
    "                        - Use lists and bullet points sparingly. Absolutely avoid nesting lists and bullet points.\n",
    "                        - Ensure to include direct inline citations for all information referenced in your answer. Use a bracketed citation style with source reference like [XXX]. If there are multiple references, provide them in separate brackets each: [XXX][ZZZ]. MAKE SURE TO USE THE REFERENCES EXACTLY AS GIVEN IN THE <reference> TAGS ABOVE.\n",
    "\n",
    "                    Write all your responses in English.\n",
    "\n",
    "                    Make sure to use Markdown in your response and structure it in this format:\n",
    "\n",
    "                    <answer>\n",
    "                    ...\n",
    "                    </answer>\n",
    "            \"\"\"},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Evaluators will expect \"answer\" and \"contexts\"\n",
    "        return {\n",
    "            #\"answer\": response.text,\n",
    "            \"answer\": response.choices[0].message.content,\n",
    "            \"contexts\": [str(doc) for doc in similar],\n",
    "        }\n",
    "\n",
    "# Example usage (Ensure `retriever` is properly initialized)\n",
    "rag_dummy = Ragdummy(retriever) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<answer>\\n9% of UK consumers are plant-based eaters, which includes both vegans and vegetarians [Document(metadata={'source': '/Users/lilian/Documents/LLM Eval Project/docs/dummy.txt'})].\\n</answer>\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_dummy.get_answer(\"What percentage of UK consumers are plant-based eaters?\")\n",
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG chain\n",
    "def predict_rag_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    response = rag_dummy.get_answer(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"]}\n",
    "\n",
    "def predict_rag_answer_with_context(example: dict):\n",
    "    \"\"\"Use this for evaluation of retrieved documents and hallucinations\"\"\"\n",
    "    response = rag_dummy.get_answer(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"], \"contexts\": response[\"contexts\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with GPT 4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Eval-4o-17704c62' at:\n",
      "https://smith.langchain.com/o/8a8f5023-8b7e-4c2c-b2bc-eb849b90cbe6/datasets/2148b077-b0a4-439d-acf9-03e55035e9ff/compare?selectedSessions=0d086311-b571-4041-a65d-2c6753c59496\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:41,  6.87s/it]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "# Evaluator\n",
    "qa_evaluator = [\n",
    "    LangChainStringEvaluator(\n",
    "        \"cot_qa\",\n",
    "        prepare_data=lambda run, example: {\n",
    "            \"prediction\": run.outputs[\"answer\"],\n",
    "            \"reference\": example.outputs[\"answer\"],\n",
    "            \"input\": example.inputs[\"question\"],\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "dataset_name = \"Market Logic Dummy\"\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    experiment_prefix=\"Eval-4o\",\n",
    "    metadata={\n",
    "    \"title\":\"Report Plant based food in the UK\",\n",
    "    \"source\": \"ProVeg International\",\n",
    "    \"publication_date\":\"2024-09-01T08:20:00Z\",\n",
    "    \"summary\":\"Market analysis of consumer attitudes and behaviors towards plant-based foods in the UK, based on survey conducted as part of the Smart Protein project.\"\n",
    "},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with GPT 4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Eval-4o-mini-714673e9' at:\n",
      "https://smith.langchain.com/o/8a8f5023-8b7e-4c2c-b2bc-eb849b90cbe6/datasets/2148b077-b0a4-439d-acf9-03e55035e9ff/compare?selectedSessions=649c9f07-3cf6-41b3-818a-9cae9a43b8df\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:32,  5.44s/it]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "# Evaluator\n",
    "qa_evaluator = [\n",
    "    LangChainStringEvaluator(\n",
    "        \"cot_qa\",\n",
    "        prepare_data=lambda run, example: {\n",
    "            \"prediction\": run.outputs[\"answer\"],\n",
    "            \"reference\": example.outputs[\"answer\"],\n",
    "            \"input\": example.inputs[\"question\"],\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "dataset_name = \"Market Logic Dummy\"\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    experiment_prefix=\"Eval-4o-mini\",\n",
    "    metadata={\n",
    "    \"title\":\"Report Plant based food in the UK\",\n",
    "    \"source\": \"ProVeg International\",\n",
    "    \"publication_date\":\"2024-09-01T08:20:00Z\",\n",
    "    \"summary\":\"Market analysis of consumer attitudes and behaviors towards plant-based foods in the UK, based on survey conducted as part of the Smart Protein project.\"\n",
    "},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with GPT 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Eval-3.5-cb93cddb' at:\n",
      "https://smith.langchain.com/o/8a8f5023-8b7e-4c2c-b2bc-eb849b90cbe6/datasets/2148b077-b0a4-439d-acf9-03e55035e9ff/compare?selectedSessions=bfdd343a-d572-4df7-bf8f-335cdb239489\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:27,  4.55s/it]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "# Evaluator\n",
    "qa_evaluator = [\n",
    "    LangChainStringEvaluator(\n",
    "        \"cot_qa\",\n",
    "        prepare_data=lambda run, example: {\n",
    "            \"prediction\": run.outputs[\"answer\"],\n",
    "            \"reference\": example.outputs[\"answer\"],\n",
    "            \"input\": example.inputs[\"question\"],\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "dataset_name = \"Market Logic Dummy\"\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    experiment_prefix=\"Eval-3.5\",\n",
    "    metadata={\n",
    "    \"title\":\"Report Plant based food in the UK\",\n",
    "    \"source\": \"ProVeg International\",\n",
    "    \"publication_date\":\"2024-09-01T08:20:00Z\",\n",
    "    \"summary\":\"Market analysis of consumer attitudes and behaviors towards plant-based foods in the UK, based on survey conducted as part of the Smart Protein project.\"\n",
    "},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with Gemini 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "# Evaluator\n",
    "qa_evaluator = [\n",
    "    LangChainStringEvaluator(\n",
    "        \"cot_qa\",\n",
    "        prepare_data=lambda run, example: {\n",
    "            \"prediction\": run.outputs[\"answer\"],\n",
    "            \"reference\": example.outputs[\"answer\"],\n",
    "            \"input\": example.inputs[\"question\"],\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "dataset_name = \"Market Logic Dummy\"\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    experiment_prefix=\"Eval-gemini-1.5-pro\",\n",
    "    metadata={\n",
    "    \"title\":\"Report Plant based food in the UK\",\n",
    "    \"source\": \"ProVeg International\",\n",
    "    \"publication_date\":\"2024-09-01T08:20:00Z\",\n",
    "    \"summary\":\"Market analysis of consumer attitudes and behaviors towards plant-based foods in the UK, based on survey conducted as part of the Smart Protein project.\"\n",
    "},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain-google-vertexai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with context Answer - GPT 4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Eval-3.5b-ad418013' at:\n",
      "https://smith.langchain.com/o/8a8f5023-8b7e-4c2c-b2bc-eb849b90cbe6/datasets/2148b077-b0a4-439d-acf9-03e55035e9ff/compare?selectedSessions=dcdb7d4b-5995-47fd-b18e-3a61f90d9cc7\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:25,  4.20s/it]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "# Evaluator\n",
    "qa_evaluator = [\n",
    "    LangChainStringEvaluator(\n",
    "        \"cot_qa\",\n",
    "        prepare_data=lambda run, example: {\n",
    "            \"prediction\": run.outputs[\"answer\"],\n",
    "            \"reference\": example.outputs[\"answer\"],\n",
    "            \"input\": example.inputs[\"question\"],\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "dataset_name = \"Market Logic Dummy\"\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    experiment_prefix=\"Eval-3.5b\",\n",
    "    metadata={\n",
    "    \"title\":\"Report Plant based food in the UK\",\n",
    "    \"source\": \"ProVeg International\",\n",
    "    \"publication_date\":\"2024-09-01T08:20:00Z\",\n",
    "    \"summary\":\"Market analysis of consumer attitudes and behaviors towards plant-based foods in the UK, based on survey conducted as part of the Smart Protein project.\"\n",
    "},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with context Answer - GPT 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Eval-4ob-99c33a6d' at:\n",
      "https://smith.langchain.com/o/8a8f5023-8b7e-4c2c-b2bc-eb849b90cbe6/datasets/2148b077-b0a4-439d-acf9-03e55035e9ff/compare?selectedSessions=957744f6-ada2-4045-9662-ae7ae3beee42\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:36,  6.03s/it]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "# Evaluator\n",
    "qa_evaluator = [\n",
    "    LangChainStringEvaluator(\n",
    "        \"cot_qa\",\n",
    "        prepare_data=lambda run, example: {\n",
    "            \"prediction\": run.outputs[\"answer\"],\n",
    "            \"reference\": example.outputs[\"answer\"],\n",
    "            \"input\": example.inputs[\"question\"],\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "dataset_name = \"Market Logic Dummy\"\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    experiment_prefix=\"Eval-4ob\",\n",
    "    metadata={\n",
    "    \"title\":\"Report Plant based food in the UK\",\n",
    "    \"source\": \"ProVeg International\",\n",
    "    \"publication_date\":\"2024-09-01T08:20:00Z\",\n",
    "    \"summary\":\"Market analysis of consumer attitudes and behaviors towards plant-based foods in the UK, based on survey conducted as part of the Smart Protein project.\"\n",
    "},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cfc4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample queries to validate evaluation pipeline\n",
    "test_queries = [\n",
    "    \"What are the key business insights from the document?\",\n",
    "    \"Summarize the main points in a structured format.\",\n",
    "    \"Are there any risks mentioned in the document?\"\n",
    "]\n",
    "\n",
    "# Running evaluations\n",
    "for query in test_queries:\n",
    "    result = rag_dummy.get_answer(query)\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(f\"Answer: {result['answer']}\\n\")\n",
    "    print(f\"Contexts: {result['contexts']}\\n\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecac0d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple evaluation metrics for responses\n",
    "def evaluate_response(response, contexts):\n",
    "    relevance_score = sum(1 for ctx in contexts if ctx in response) / len(contexts) if contexts else 0\n",
    "    length_score = len(response.split())  # Word count as a rough metric\n",
    "    return {\"relevance_score\": relevance_score, \"length_score\": length_score}\n",
    "\n",
    "# Run evaluation on test queries\n",
    "for query in test_queries:\n",
    "    result = rag_dummy.get_answer(query)\n",
    "    scores = evaluate_response(result['answer'], result['contexts'])\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(f\"Relevance Score: {scores['relevance_score']}\")\n",
    "    print(f\"Response Length: {scores['length_score']} words\\n\")\n",
    "    print(\"-\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
